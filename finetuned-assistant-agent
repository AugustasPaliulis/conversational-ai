{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.57.1\n!pip install trl==0.24.0\n!pip install peft==0.16.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check versions\nimport transformers, trl, peft\nprint(\"transformers:\", transformers.__version__)\nprint(\"trl:\", trl.__version__)\nprint(\"peft:\", peft.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:49:37.352961Z","iopub.execute_input":"2025-12-06T13:49:37.353247Z","iopub.status.idle":"2025-12-06T13:49:37.357880Z","shell.execute_reply.started":"2025-12-06T13:49:37.353219Z","shell.execute_reply":"2025-12-06T13:49:37.357037Z"}},"outputs":[{"name":"stdout","text":"transformers: 4.57.1\ntrl: 0.24.0\npeft: 0.16.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np \nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nfrom typing import List\nfrom datasets import Dataset\nimport torch\nimport random\ntorch.manual_seed(3407); random.seed(3407); np.random.seed(3407)\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig, get_peft_model, PeftModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:50:26.960932Z","iopub.execute_input":"2025-12-06T13:50:26.961692Z","iopub.status.idle":"2025-12-06T13:50:26.967790Z","shell.execute_reply.started":"2025-12-06T13:50:26.961657Z","shell.execute_reply":"2025-12-06T13:50:26.967058Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def setup_repo(repo_url: str, repo_name: str, work_dir: str = \"/kaggle/working\"):\n    os.chdir(work_dir)\n    \n    # Remove repo if it exists\n    if os.path.exists(os.path.join(work_dir, repo_name)):\n        shutil.rmtree(os.path.join(work_dir, repo_name))\n    \n    # Clone repo\n    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n    \n    # Move into repo/data\n    os.chdir(os.path.join(repo_name, \"data\"))\n\n\nsetup_repo(\"https://github.com/lkra/dstc11-track5.git\", \"dstc11-track5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:50:29.700999Z","iopub.execute_input":"2025-12-06T13:50:29.701846Z","iopub.status.idle":"2025-12-06T13:50:33.393236Z","shell.execute_reply.started":"2025-12-06T13:50:29.701816Z","shell.execute_reply":"2025-12-06T13:50:33.392636Z"}},"outputs":[{"name":"stderr","text":"Cloning into 'dstc11-track5'...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"## List all files in the current directory iteratively:\nfor dirname, _, filenames in os.walk('.'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:51:07.432897Z","iopub.execute_input":"2025-12-06T13:51:07.433543Z","iopub.status.idle":"2025-12-06T13:51:07.439186Z","shell.execute_reply.started":"2025-12-06T13:51:07.433516Z","shell.execute_reply":"2025-12-06T13:51:07.438427Z"}},"outputs":[{"name":"stdout","text":"./knowledge.json\n./output_schema.json\n./README.md\n./knowledge_aug_reviews.json\n./knowledge_aug_domain_reviews.json\n./test/labels.json\n./test/logs.json\n./train/labels.json\n./train/logs_bkp.json\n./train/logs.json\n./train/bkp/labels.json\n./train/bkp/logs.json\n./val/labels.json\n./val/logs.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open('train/logs.json', 'r') as f:\n    train_ds=json.load(f)\n\nwith open('train/labels.json', 'r') as f:\n    labels=json.load(f)\n\nwith open('knowledge.json', 'r') as f:\n    knowledge_base=json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:52:16.795721Z","iopub.execute_input":"2025-12-06T13:52:16.795994Z","iopub.status.idle":"2025-12-06T13:52:17.203784Z","shell.execute_reply.started":"2025-12-06T13:52:16.795975Z","shell.execute_reply":"2025-12-06T13:52:17.203093Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def format_dialogue(dialogue: List[dict]) -> List[dict]: \n    \"\"\"\n    Args:\n    dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n        - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n        - 'text' (str): The text spoken by the respective speaker.\n\n    Returns:\n        List[dict]: A new array with a specific role and content\n\n    \"\"\"\n    # Your solution here\n    messages=[]\n    for dialogue_element in dialogue:\n        try:\n            \n            role = 'user' if dialogue_element['speaker'] == 'U' else 'assistant'\n            messages.append({\"role\": role, \"content\": dialogue_element['text']})\n        except Exception as e:\n            \n            continue\n\n\n    return messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:28:03.777966Z","iopub.execute_input":"2025-12-06T14:28:03.778520Z","iopub.status.idle":"2025-12-06T14:28:03.783756Z","shell.execute_reply.started":"2025-12-06T14:28:03.778494Z","shell.execute_reply":"2025-12-06T14:28:03.782807Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"* **reformat_dataset:** this function formats our dataset into a structure suitable for conversational language-model training. For each sample, it extracts the dialogue and its associated response, appends the response as a message with the \"system\" role, and adds the resulting message list to a new dataset. Samples that cause errors are skipped. The final output is the Hugging Face Dataset, containing conversations organized as lists of role-content message objects.\n\nNOTE: This function is slightly different from the one in Assignment 1. Here, you only need to access the dialogue and the response.\n","metadata":{}},{"cell_type":"code","source":" system_prompt = ({\n    \"role\": \"system\",\n    \"content\": (\n        \"You are a specialized assistant that helps users with questions and recommendations \"\n        \"about hotels and restaurants.\\n\\n\"\n        \"Core behavior:\\n\"\n        \"- Always be accurate, concrete, and non-generic. Use clear, direct language.\\n\"\n        \"- If system messages include reviews, ratings, or metadata about specific hotels/restaurants, \"\n        \"treat that information as your primary source of truth.\\n\"\n        \"- When recommending, always explain *why* you recommend something, referring to the \"\n        \"evidence in the reviews (location, cleanliness, service, noise, price/quality, etc.).\\n\"\n        \"- When the data is mixed or conflicting, say so explicitly and describe the trade-offs.\\n\"\n        \"- If the user’s preferences are clear (budget, location, style, amenities), tailor all \"\n        \"answers to those preferences. If they are not clear and this matters for the answer, ask \"\n        \"1–2 targeted clarifying questions.\\n\"\n        \"- Do not invent specific facts (prices, exact distances, facilities, policies) that are \"\n        \"not supported by the provided data or by general, well-known domain knowledge. If you \"\n        \"don’t know, say that you don’t know.\\n\"\n        \"- Be concise but complete: prioritize usefulness over verbosity.\\n\\n\"\n        \"Handling provided reviews (from additional system messages):\\n\"\n        \"- Assume all provided reviews are about real stays or visits unless clearly marked otherwise.\\n\"\n        \"- Do not quote long passages verbatim; instead, summarize and synthesize key points.\\n\"\n        \"- Pay attention to recency if timestamps are available; more recent reviews should carry \"\n        \"slightly more weight than very old ones, especially for service quality or renovation issues.\\n\"\n        \"- If no reviews are provided for a requested place, state that you have no review data for \"\n        \"it and answer in general terms only if appropriate.\\n\\n\"\n        \"Safety and honesty:\\n\"\n        \"- Never fabricate user reviews or claim that a sentiment is in the reviews if it is not.\\n\"\n        \"- Clearly distinguish between facts from the provided data and general expectations or \"\n        \"educated guesses about typical hotels/restaurants.\\n\"\n        \"- If the user requests something impossible or outside your scope, explain the limitation \"\n        \"plainly and suggest the closest helpful alternative.\\n\"\n    )\n}) # TODO: Improve the system prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:28:05.933189Z","iopub.execute_input":"2025-12-06T14:28:05.933890Z","iopub.status.idle":"2025-12-06T14:28:05.938660Z","shell.execute_reply.started":"2025-12-06T14:28:05.933851Z","shell.execute_reply":"2025-12-06T14:28:05.937949Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from datasets import Dataset\n\ndef extract_sources(knowledge, knowledge_base):\n    grouped = {}  # entity_id → list of sentences\n\n    for k in knowledge:\n        try:\n            domain = k[\"domain\"]\n            entity_id = str(k[\"entity_id\"])\n            doc_id = str(k[\"doc_id\"])\n            sent_id = str(k[\"sent_id\"])\n\n            entity = knowledge_base[domain][entity_id]\n            entity_name = entity[\"name\"]\n\n            sentence = (\n                entity[\"reviews\"][doc_id][\"sentences\"][sent_id]\n            )\n\n            # Initialize list if new entity\n            if entity_id not in grouped:\n                grouped[entity_id] = {\n                    \"name\": entity_name,\n                    \"sentences\": []\n                }\n\n            grouped[entity_id][\"sentences\"].append(sentence)\n\n        except Exception as e:\n            continue\n\n    # Build final output\n    sources = []\n    for entity_id, data in grouped.items():\n        name = data[\"name\"]\n        sentences = data[\"sentences\"]\n\n        # Name appears once at the beginning\n        combined = \"Name of restaurant/hotel: \" + name + \" — \" + \" \".join(sentences)\n        sources.append(combined)\n\n    return sources\n\n\ndef reformat_dataset(dialogue_dataset, labels_dataset, knowledge_base):\n    \"\"\"\n    Turn raw dialogue + labels + knowledge into a list of dicts:\n\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"<reviews...>\"},   # only if knowledge exists\n            {\"role\": \"user\", \"content\": \"...\"},\n            {\"role\": \"assistant\", \"content\": \"...\"},\n            ...\n            {\"role\": \"assistant\", \"content\": \"<golden response>\"},\n        ]\n    }\n    \"\"\"\n    reformatted = []\n\n    for index in range(len(dialogue_dataset)):\n        try:\n            # 1) dialogue → list[{\"role\",\"content\"}]\n            dialogue = dialogue_dataset[index]\n            sample_dialogue = format_dialogue(dialogue)\n\n            # 2) labels\n            label_item = labels_dataset[index]\n            sample_response = label_item[\"response\"]\n\n        except (IndexError, KeyError, TypeError):\n            # Dialogue/label misalignment or broken entry -> skip this sample\n            continue\n\n        # 3) knowledge → sources (list of review strings)\n        raw_knowledge = label_item.get(\"knowledge\", []) or []\n        \n        sources = extract_sources(raw_knowledge, knowledge_base)\n        # 4) build messages: knowledge (system) + dialogue + golden assistant response\n        messages = []\n        messages.append(system_prompt)\n        if sources:\n            kb_block = \" \".join(sources)\n            messages.append({\n                \"role\": \"system\",\n                \"content\": \"Relevant reviews for this query:\\n\\n\" + kb_block,\n            })\n\n        # dialogue history\n        messages.extend(sample_dialogue)\n\n        # golden answer as assistant\n        messages.append({\n            \"role\": \"assistant\",\n            \"content\": sample_response,\n        })\n\n        reformatted.append({\"messages\": messages})\n\n    return reformatted\n\n\n# Build HF dataset from train split\nreformatted_list = reformat_dataset(train_ds, labels, knowledge_base)\ndataset = Dataset.from_list(reformatted_list)\n\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:28:19.889063Z","iopub.execute_input":"2025-12-06T14:28:19.889733Z","iopub.status.idle":"2025-12-06T14:28:20.897513Z","shell.execute_reply.started":"2025-12-06T14:28:19.889704Z","shell.execute_reply":"2025-12-06T14:28:20.896848Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['messages'],\n    num_rows: 16897\n})"},"metadata":{}}],"execution_count":50},{"cell_type":"markdown","source":"From this point on, we have already created the dataset for our training dataset. In the following function, you need to do the same for the validation and test datasets.\n\n* **process_dataset_split:** based on the input string of the function, load the corresponding split and create the dataset using the same process as for the training data.","metadata":{}},{"cell_type":"code","source":"def process_dataset_split(split: str) -> Dataset: \n    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n\n    This function loads a dataset split (e.g., 'val', 'test') and generates a dataset for it, similar to what we had for the train split.\n\n    Args:\n        split (str): The name of the dataset split to process\n\n    Returns:\n        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n\n    \"\"\"\n    # Your solution here.\n    if split == 'test':\n        with open('test/logs.json', 'r') as f:\n            test_ds=json.load(f)\n        \n        with open('test/labels.json', 'r') as f:\n            test_labels=json.load(f)\n\n    \n        return Dataset.from_list(reformat_dataset(test_ds, test_labels, knowledge_base))\n\n    elif split == 'val':\n        with open('val/logs.json', 'r') as f:\n            val_ds=json.load(f)\n        \n        with open('val/labels.json', 'r') as f:\n            val_labels=json.load(f)\n        \n        return Dataset.from_list(reformat_dataset(val_ds, val_labels, knowledge_base))\n\n    raise ValueError('Split is not correct, it must be either \"val\" or \"test\"')\n\nvalidation_ds = process_dataset_split(\"val\")\ntest_ds = process_dataset_split(\"test\")\n\nvalidation_ds, test_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:29:58.914019Z","iopub.execute_input":"2025-12-06T14:29:58.914340Z","iopub.status.idle":"2025-12-06T14:29:59.312604Z","shell.execute_reply.started":"2025-12-06T14:29:58.914289Z","shell.execute_reply":"2025-12-06T14:29:59.311849Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['messages'],\n     num_rows: 2129\n }),\n Dataset({\n     features: ['messages'],\n     num_rows: 2798\n }))"},"metadata":{}}],"execution_count":54},{"cell_type":"markdown","source":"## Fine-Tuning\n\n### Setting up\n\nNow that we have our data ready, we can look at fine-tuning our model. We will be using HuggingFace Transformers. \n\nAs a language model, we will be using __Qwen3-1.7B__. Let's load it!","metadata":{}},{"cell_type":"code","source":"model_id = \"Qwen/Qwen3-1.7B\"\ntok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nbase = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:30:04.084856Z","iopub.execute_input":"2025-12-06T14:30:04.085123Z","iopub.status.idle":"2025-12-06T14:30:17.884984Z","shell.execute_reply.started":"2025-12-06T14:30:04.085106Z","shell.execute_reply":"2025-12-06T14:30:17.884095Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e74346eda248c9bcb4455b58979991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad06d3bf4844a209971a86296988bfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d1808260794a598c19abe8bbee4889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052fc1cef96c426ca177acc06ec177b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ec696f2966427fbaaa122b2a9d9c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa643c0d3a744d64ac8aaf7fc7e4c8f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b839875636774c51befbd4db0098a701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8fd578822f64af0aa72e4d9339c4d92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b31c10545dde4218a94dcad495b8efe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b0f65742754ac1be0215750d1308be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f322816f8f4f8d99f766c274b24061"}},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"peft_cfg = LoraConfig(r=16, \n           lora_alpha=32, \n           lora_dropout = 0.05, \n           bias = \"none\", \n           use_rslora = False, \n           target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"])\n\nmodel = get_peft_model(base, peft_cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:30:37.067385Z","iopub.execute_input":"2025-12-06T14:30:37.068116Z","iopub.status.idle":"2025-12-06T14:30:38.023699Z","shell.execute_reply.started":"2025-12-06T14:30:37.068092Z","shell.execute_reply":"2025-12-06T14:30:38.023018Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def pick_bf16():\n    if torch.cuda.is_available():\n        major, _ = torch.cuda.get_device_capability()\n        return major >= 8\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:30:40.071880Z","iopub.execute_input":"2025-12-06T14:30:40.072487Z","iopub.status.idle":"2025-12-06T14:30:40.077111Z","shell.execute_reply.started":"2025-12-06T14:30:40.072463Z","shell.execute_reply":"2025-12-06T14:30:40.076390Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"NUM_TRAIN_EPOCHS = 2\nLEARNING_RATE    = 0.0003\nWARMUP_STEPS     = 300\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nsft_args = SFTConfig(\n    output_dir=\"outputs\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    logging_steps=10,\n    lr_scheduler_type=\"linear\",\n    weight_decay=0.01,\n    max_length=1024,\n    optim=\"adamw_torch_fused\",\n    fp16=not pick_bf16(),\n    bf16=pick_bf16(),\n    packing=False,\n    dataset_num_proc=2,\n    report_to=\"none\",\n    seed=3407\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:34:38.842958Z","iopub.execute_input":"2025-12-06T14:34:38.843542Z","iopub.status.idle":"2025-12-06T14:34:38.878659Z","shell.execute_reply.started":"2025-12-06T14:34:38.843519Z","shell.execute_reply":"2025-12-06T14:34:38.878026Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"Let’s define our trainer object using the parameters we set earlier.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,      \n    eval_dataset=validation_ds,\n    args=sft_args,\n    processing_class=tok\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:34:41.053557Z","iopub.execute_input":"2025-12-06T14:34:41.053830Z","iopub.status.idle":"2025-12-06T14:35:20.634222Z","shell.execute_reply.started":"2025-12-06T14:34:41.053810Z","shell.execute_reply":"2025-12-06T14:35:20.633214Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/16897 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e6e272294642278d6ec7f906403395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset (num_proc=2):   0%|          | 0/16897 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86f219366bd34386a60440ba235c13bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset (num_proc=2):   0%|          | 0/2129 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d7a0d8268944459cab846f2bb41b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset (num_proc=2):   0%|          | 0/2129 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f9b846b8aa48f48faa498b5161dc97"}},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"Now, let’s train the model. Depending on the number of epochs, the process may take between 2 to 6 hours.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:35:46.740230Z","iopub.execute_input":"2025-12-06T14:35:46.741227Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3700' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3700/4226 7:39:37 < 1:05:22, 0.13 it/s, Epoch 1.75/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>4.250200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.755400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.038100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.533800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.029500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.348400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.976500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.874500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.820300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.805300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.779400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.801500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.709800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.722400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.705100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.752800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.681400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.683500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.685900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.707100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.628900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.635000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.668900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.626200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.682800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.669700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.670400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.657200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.645600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.596200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.635500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.637500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.587000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.611400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.602600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.598000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.563800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.558600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.581500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.575600</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.598900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.574600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.575000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.581100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.540500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.542600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.564600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.576000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.564200</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.579000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.573100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.517200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.531400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.545100</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.557500</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.527800</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.523600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.534000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.527700</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.515000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.541600</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.533000</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.535800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.531800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.505500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.512500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.517500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.513800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.479500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.505500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.486700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.528800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.497400</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.513500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.488300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.523900</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.488000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.460700</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.492400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.487300</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.473200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.465500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.473600</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.484300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.465500</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.466200</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.509700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.472800</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.474600</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.490200</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.476700</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.504000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.476000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.474600</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.464800</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.475100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.493100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.466000</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.510800</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.473400</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.462100</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.476800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.445800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.454200</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.464100</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.481300</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.442900</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.449600</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.450400</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.432900</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.464600</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.473200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.425900</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.443700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.433600</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.426100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.482300</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.452900</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.418400</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.444000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.418800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.422800</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.445000</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.467000</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.425700</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.447300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.445800</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.432600</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.442400</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.457700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.458500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.455600</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.405000</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.429700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.422600</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.432200</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.433300</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.410300</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.439500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.423500</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.437600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.433000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.403900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.416100</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.414500</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.411100</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.431700</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.429300</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.411800</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.438400</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.433200</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.399300</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.397500</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.401400</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.433600</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.430700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.408900</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.431400</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.409500</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.407700</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.429000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.397100</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.410100</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.425300</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.399600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.419500</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.398700</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.406000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.375800</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.374800</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.416300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.417600</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.404500</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.364100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.387800</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.431300</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.415300</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.378700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.391600</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.383800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.386500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.411300</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.402700</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.412800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.411400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.430900</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.392100</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.394000</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.419100</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.369100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.412100</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.377300</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.380800</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.388800</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.374100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.387700</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.419100</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.363100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.404600</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.405600</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.342700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.397700</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.392800</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.373600</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.339400</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.367600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.365600</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.367600</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.371200</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.355300</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.336200</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.360500</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.349900</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.387800</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.357700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.366400</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.397400</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.357000</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.357500</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.365100</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.338600</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.363100</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.342300</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.325900</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.361400</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.342600</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.368800</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.375700</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.365400</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.374000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.362800</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.351200</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.347700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.337000</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.360200</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.370700</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.347500</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.345400</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.349300</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.354100</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.352100</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.359100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.332700</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.336800</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.351600</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.359900</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.345700</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.357100</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.352200</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.357000</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.330400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.346900</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.362400</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.354100</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.329400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.357000</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.327200</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.361500</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.346900</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.352100</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.356500</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.356800</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.335400</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.341900</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.308800</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.337600</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.342900</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.346400</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.341500</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.339600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.353500</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.347200</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.351500</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.339000</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.342700</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.333300</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.347800</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.324500</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.333500</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.337000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.350500</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.345600</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.317500</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.329500</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.339300</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.321300</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.333100</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.340700</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.339700</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.326500</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.323000</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.316900</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.327000</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.329800</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.335300</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.343900</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.335600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.328700</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.331400</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.309500</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.323000</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.339900</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.322400</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.337900</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.350600</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.354600</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.343900</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.332600</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.331800</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.324300</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.326500</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.335100</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.302700</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.306500</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.316300</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.330400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.309500</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.325800</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.307400</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.322200</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.328700</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.337700</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.323300</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.319700</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.323900</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.329500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.347900</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.324100</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.316900</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.339400</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.310200</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.349400</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.340900</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.350900</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.329700</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.339700</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.328700</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.317800</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.339400</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.329000</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.330300</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.339800</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.290300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Finally, save your trained model in the output/adapter directory.\n\nYou can then download it for future use.","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"outputs/adapter\")  \ntok.save_pretrained(\"outputs/adapter\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Now let's take a look at what our fine-tuned model does! ","metadata":{}},{"cell_type":"code","source":"base_model_id = \"Qwen/Qwen3-1.7B\"\nadapter_path  = \"outputs/adapter\" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the base model.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\nmodel_base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")\nmodel_base_for_adapter = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=\"auto\", device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, load the finetuned model from outputs/adapter.","metadata":{}},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model_base_for_adapter, adapter_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's generate outputs for a dialog using the base model and the finetuned model.","metadata":{}},{"cell_type":"code","source":"dialogue = test_ds[0]['messages'][:-1]\nresponse = test_ds[0]['messages'][-1]\n\ndialogue, response","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=True, enable_thinking=False)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model_base.device)\n\ngenerated_ids = model_base.generate(**model_inputs, max_new_tokens=500)\noutput_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n\nprint(\"Base Model: \", tokenizer.decode(output_ids, skip_special_tokens=True).strip())\nprint(\"Ground-truth: \", response[\"content\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=True, enable_thinking=False)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=500)\noutput_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n\nprint(\"Finetuned Model: \", tokenizer.decode(output_ids, skip_special_tokens=True).strip())\nprint(\"Ground-truth: \", response[\"content\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyze Model Responses\nNow that we have our model up and running, let's look at what types of generations the fine-tuned model produces and compare them with the generations of the vanilla model. For this, we will use samples from the test set. Pick the first 10 samples from the __test dataset__ for the analysis and use those to answer the questions below. \n\nNote: Please print the outputs of these 10 examples.\n\n#### Questions\n\nPlease answer each question with a single paragraph, consisting of 2-5 sentences. Include examples to support your answers.\n","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q3:__ What values for the following hyperparameters did you set: number of epochs, learning rate, and warmup steps? Explain briefly why you chose these parameters. </span>\n","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q4:__ What is the training loss you achieved, what is its trend over the iterations, and what does this mean for your model? </span>","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q5:__ Compare the off-the-shelf to the fine-tuned model: What do you notice in terms of quality of the generation? Are the generations responding to the query?</span> \n","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q6:__ Repeat the experiment with a different hyperparameter value for number of epochs, learning rate, or warmup steps. Fine-tune the model using these new settings, and describe your observations regarding the training loss, as well as the quality of the model’s answers to the questions from the test dataset.</span> \n","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q7:__ How does the fine-tuned model perform in the following scenarios? Compare this to your answer about the vanilla model in assignment 1.</span> \n1. When all reviews agree with each other\n2. When only one review disagrees\n3. When opinions in the reviews are mixed (i.e. high disagreement)","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q8:__ How does the length of the dialogue/conversation history affect the fine-tuned model's generation? Compare this to your answer about the vanilla model in assignment 1.</span> ","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"background:yellow\">__Q9:__ Create 5 samples that come from a different domain (e.g., airports, shops, web-stores; you can use an LLM or search the web for this). How does the fine-tuned LLM perform on these, compared to the dialogues in the DSTC-11 task5 domains?</span> ","metadata":{}},{"cell_type":"markdown","source":"```\n# Your answer here\n```","metadata":{}}]}